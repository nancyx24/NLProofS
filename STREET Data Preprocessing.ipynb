{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing STREET Data"
      ],
      "metadata": {
        "id": "0wHOIfE1WGyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data for GSM8K and SCONE are downloaded from https://github.com/amazon-science/street-reasoning/tree/main/data."
      ],
      "metadata": {
        "id": "LBfS40W7CVlE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5YpV6x843sG"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "import json\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing gms8k dataset to apply EntailmentBank functions\n",
        "def gsm8kFormatter(input_path: str, output_path: str):\n",
        "  file = open(input_path)\n",
        "\n",
        "  with open(output_path, \"w\") as outfile:\n",
        "    for line in file:\n",
        "      ex = json.loads(line)\n",
        "      \n",
        "      # delete irrelevant keys\n",
        "      not_needed_keys = ['context', 'options', 'rationale', 'metadata', 'reasoning_graph_edges', 'textual_logical_units']\n",
        "      for k in not_needed_keys:\n",
        "        if k in ex:\n",
        "          del ex[k]\n",
        "  \n",
        "      # modify hypothesis\n",
        "      if 'answer' in ex:\n",
        "        ex['hypothesis'] = 'The answer is ' + str(ex['answer'])\n",
        "      \n",
        "      # modify context\n",
        "      if 'linearized_input' in ex:\n",
        "        ex['context'] = ex['linearized_input']\n",
        "        del ex['linearized_input']\n",
        "      \n",
        "      # modify proof\n",
        "      if 'linearized_output' in ex:\n",
        "        ex['proof'] = re.sub(r'int.: ' + ex['hypothesis'], 'hypothesis', ex['linearized_output'])\n",
        "        del ex['linearized_output']\n",
        "\n",
        "      # modify meta and answer for evaluation\n",
        "      ex['meta'] = {'triples': '' , 'distractors': '', 'intermediate_conclusions': ''}\n",
        "\n",
        "      # get triples\n",
        "      split_steps = ex['context'].split('sent')\n",
        "      triples = {}\n",
        "\n",
        "      for step in split_steps:\n",
        "        if step == '':\n",
        "          continue\n",
        "        ident = 'sent' + step.split(\": \")[0]\n",
        "        sent = step.split(\": \")[-1].strip()\n",
        "        triples[ident] = sent\n",
        "\n",
        "      ex['meta']['triples'] = triples\n",
        "\n",
        "      ex['question'] = re.sub(r'\\n', '', ex['question'])\n",
        "      # ex['meta']['question'] = ex['question']\n",
        "      ex['answer'] = ex['hypothesis']\n",
        "\n",
        "      # get intermediate_conclusions\n",
        "      implications = ex['proof'].split(\";\")\n",
        "      intermediates = {}\n",
        "      for i in implications:\n",
        "        interim = i.split(\" -> \")[-1]\n",
        "        interim_list = interim.split(\": \")\n",
        "\n",
        "        # bad\n",
        "        if 'hypothesis' in interim_list[-1]:\n",
        "          continue\n",
        "        if 'int' not in interim_list[0]:\n",
        "          continue\n",
        "\n",
        "        intermediates[interim_list[0].strip()] = interim_list[1].strip()\n",
        "      \n",
        "      ex['meta']['intermediate_conclusions'] = intermediates\n",
        "      \n",
        "      # remove invalid examples\n",
        "      if 'hypothesis' in ex['proof']:\n",
        "        if 'sent0' not in ex['proof']:\n",
        "          json.dump(ex, outfile)\n",
        "          outfile.write('\\n')   "
      ],
      "metadata": {
        "id": "P67sn1CqEfyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# format gms8k data\n",
        "gsm8kFormatter('/content/sample_data/raw_gsm8k_train.jsonl', 'train_gsm8k_interim.jsonl')\n",
        "gsm8kFormatter('/content/sample_data/raw_gsm8k_dev.jsonl', 'dev_gsm8k_interim.jsonl')\n",
        "gsm8kFormatter('/content/sample_data/raw_gsm8k_test.jsonl', 'test_gsm8k_interim.jsonl')"
      ],
      "metadata": {
        "id": "fejTKzEEE2W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reduces gsm8k dataset to desired size\n",
        "def gsm8kCutter(input_path: str, output_path: str, samples: int):\n",
        "  file = open(input_path)\n",
        "\n",
        "  with open(output_path, \"w\") as outfile:\n",
        "    count = 0\n",
        "\n",
        "    for line in file:\n",
        "      ex = json.loads(line)\n",
        "      count += 1\n",
        "      if count > samples:\n",
        "        break\n",
        "      \n",
        "      json.dump(ex, outfile)\n",
        "      outfile.write('\\n')"
      ],
      "metadata": {
        "id": "GoWLWBxHSRH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reduces gsm8k dataset\n",
        "gsm8kCutter('train_gsm8k_interim.jsonl', 'train_gsm8k.jsonl', 250)\n",
        "gsm8kCutter('dev_gsm8k_interim.jsonl', 'dev_gsm8k.jsonl', 50)\n",
        "gsm8kCutter('test_gsm8k_interim.jsonl', 'test_gsm8k.jsonl', 75)"
      ],
      "metadata": {
        "id": "BoarGH1tTEib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function for reduced_proof function\n",
        "# used to find relevant proof steps for desired portion of hypothesis\n",
        "def recursion(steps, reverse_implication, consequent_step, antecedents):\n",
        "  # base case\n",
        "  if not any(\"int\" in a for a in antecedents):\n",
        "    return\n",
        "  \n",
        "  for el in antecedents:\n",
        "    if \"int\" in el:\n",
        "      steps.append(consequent_step[el].strip())\n",
        "      recursion(steps, reverse_implication, consequent_step, reverse_implication[el])"
      ],
      "metadata": {
        "id": "sZDth3UTnSaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# used to find relevant proof steps for desired portion of hypothesis\n",
        "# type is alchemy, scene, or tangram\n",
        "def reduced_proof(text, problem_type):\n",
        "  premises = text.split(\";\")\n",
        "  copy = text.split(\";\")\n",
        "\n",
        "  if problem_type == 'alchemy':\n",
        "    key = premises[-2].split(\" -> \")[-1].split(\": \")[-1].split(\" \")[0]\n",
        "  if problem_type == 'scene' or 'tangram':\n",
        "    key = premises[-2].strip().split(\":\")[-1].split('has')[0].strip() + ' '\n",
        "\n",
        "  premises.reverse() # reverse premises for loop\n",
        "  antecedents = []\n",
        "  reverse_implication = {} # key: consequent; value: antecedent labels\n",
        "  consequent_step = {} # key: consequent; value: entire implication step\n",
        "  steps = [] # all proof steps that lead to desired portion of hypothesis\n",
        "\n",
        "  # build reverse_implication dictionary\n",
        "  for p in premises:\n",
        "    ac = p.split(\" -> \")\n",
        "\n",
        "    if len(ac) < 2:\n",
        "      continue\n",
        "    \n",
        "    antecedents = ac[0].strip().split(\" & \")\n",
        "    consequent = ac[1].split(\": \")[0].strip()\n",
        "    reverse_implication[consequent] = antecedents\n",
        "    consequent_step[consequent] = p\n",
        "\n",
        "  # find all premises with desired portion of hypothesis\n",
        "  for p in premises:\n",
        "    ac = p.split(\" -> \")\n",
        "\n",
        "    # check for empty strings\n",
        "    if len(ac) < 2:\n",
        "      continue\n",
        "\n",
        "    # check if relevant to our hypothesis\n",
        "    if problem_type == 'alchemy':\n",
        "      value = ac[1].split(\": \")[1].split(\" \")[0] # beaker number\n",
        "    if problem_type == 'scene' or 'tangram':\n",
        "      value = ac[1].split(\":\")[-1].split('has')[0].strip() + ' ' # position number\n",
        "\n",
        "    # if premise relevant, recursively find all previous implications to this step\n",
        "    consequent = ac[1].split(\": \")[0].strip()\n",
        "    if value == key:\n",
        "      steps.append(p.strip())\n",
        "      antecedents = reverse_implication[consequent]\n",
        "      recursion(steps, reverse_implication, consequent_step, antecedents)\n",
        "\n",
        "  # process output\n",
        "  steps = list(set(steps))\n",
        "  output_list = []\n",
        "\n",
        "  for c in copy:\n",
        "    if c.strip() in steps:\n",
        "      output_list.append(c.strip())\n",
        "\n",
        "  # concatenate into a single output string\n",
        "  output = \"\"\n",
        "  for i in output_list:\n",
        "    output = output + i + \"; \"\n",
        "  \n",
        "  return key, output"
      ],
      "metadata": {
        "id": "46pp8NMfnRly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocesses scone dataset to apply EntailmentBank functions\n",
        "def sconeFormatter(input_path: str, output_path: str):\n",
        "  file = open(input_path)\n",
        "  with open(output_path, \"w\") as outfile:\n",
        "    for line in file:\n",
        "      ex = json.loads(line)\n",
        "\n",
        "      # casework by type of problem\n",
        "      if 'ALCHEMY' in ex['id']:\n",
        "        problem_type = 'alchemy'\n",
        "      elif 'SCENE' in ex['id']:\n",
        "        problem_type = 'scene'\n",
        "      elif 'TANGRAM' in ex['id']:\n",
        "        problem_type = 'tangram'\n",
        "      else:\n",
        "        # erroneous ids\n",
        "        continue\n",
        "\n",
        "      # delete irrelevant keys\n",
        "      not_needed_keys = ['context', 'options', 'rationale', 'textual_logical_units', 'metadata', 'reasoning_graph_edges']\n",
        "      for k in not_needed_keys:\n",
        "        if k in ex:\n",
        "          del ex[k]\n",
        "\n",
        "      # modify context\n",
        "      ex['context'] = ex['linearized_input']\n",
        "      del ex['linearized_input']\n",
        "      \n",
        "      # modify proof, extract last beaker or position\n",
        "      proof = ex['linearized_output']\n",
        "      premises = proof.split(\";\")\n",
        "      key, ex['proof'] = reduced_proof(proof, problem_type)\n",
        "      del ex['linearized_output']\n",
        "\n",
        "      # answer is answer for last beaker or position\n",
        "      # hypothesis is answer in natural language\n",
        "      if 'answer' in ex:\n",
        "        # remove faulty example when answer is incomplete\n",
        "        if key not in ex['answer']:\n",
        "          continue\n",
        "        all_hypotheses = ex['answer'].split(\";\")\n",
        "        for h in all_hypotheses:\n",
        "          if key in h:\n",
        "            ex['hypothesis'] = h.strip()\n",
        "        \n",
        "        del ex['answer']\n",
        "      \n",
        "      # modify proof so that last step is hypothesis\n",
        "      ex['proof'] = re.sub(r'int[0-9]+: ' + ex['hypothesis'], 'hypothesis', ex['proof'])\n",
        "\n",
        "      # remove faulty example when hypothesis is intermediary step\n",
        "      if ex['proof'].count('hypothesis') > 1:\n",
        "        continue\n",
        "      \n",
        "      # modify meta and answer for evaluation\n",
        "      ex['meta'] = {'triples': '' , 'distractors': '', 'intermediate_conclusions': ''}\n",
        "\n",
        "      # get triples\n",
        "      split_steps = ex['context'].split('sent')\n",
        "      triples = {}\n",
        "\n",
        "      for step in split_steps:\n",
        "        if step == '':\n",
        "          continue\n",
        "        ident = 'sent' + step.split(\": \")[0]\n",
        "        sent = step.split(\": \")[-1].strip()\n",
        "        triples[ident] = sent\n",
        "\n",
        "      ex['meta']['triples'] = triples\n",
        "\n",
        "      ex['question'] = re.sub(r'\\n', '', ex['question'])\n",
        "      # ex['meta']['question'] = ex['question']\n",
        "      ex['answer'] = ex['hypothesis']\n",
        "\n",
        "      # get intermediate_conclusions\n",
        "      implications = ex['proof'].split(\";\")\n",
        "      intermediates = {}\n",
        "      for i in implications:\n",
        "        interim = i.split(\" -> \")[-1]\n",
        "        interim_list = interim.split(\": \")\n",
        "\n",
        "        # bad\n",
        "        if 'hypothesis' in interim_list[-1]:\n",
        "          continue\n",
        "        if 'int' not in interim_list[0]:\n",
        "          continue\n",
        "\n",
        "        intermediates[interim_list[0].strip()] = interim_list[1].strip()\n",
        "      \n",
        "      ex['meta']['intermediate_conclusions'] = intermediates\n",
        "\n",
        "      # get distractors\n",
        "      premises = ex['context'].split(\"sent\")\n",
        "      ident = []\n",
        "      distractors = []\n",
        "\n",
        "      for sent in premises:\n",
        "        if sent == '':\n",
        "          continue\n",
        "\n",
        "        p = 'sent' + sent.split(': ')[0]\n",
        "        ident.append(p)\n",
        "\n",
        "      for i in ident:\n",
        "        if i + \" \" not in ex['proof']:\n",
        "          distractors.append(i)\n",
        "      \n",
        "      ex['meta']['distractors'] = distractors\n",
        "\n",
        "      json.dump(ex, outfile)\n",
        "      outfile.write('\\n')"
      ],
      "metadata": {
        "id": "e64japyVm6FM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate interim datasets\n",
        "sconeFormatter('/content/sample_data/raw_scone_train.jsonl', 'train_scone_interim.jsonl')\n",
        "sconeFormatter('/content/sample_data/raw_scone_dev.jsonl', 'dev_scone_interim.jsonl')\n",
        "sconeFormatter('/content/sample_data/raw_scone_test.jsonl', 'test_scone_interim.jsonl')"
      ],
      "metadata": {
        "id": "6A-3pPLusenc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reduces scone dataset to desired size\n",
        "def sconeCutter(input_path: str, output_path: str, samples):\n",
        "  file = open(input_path)\n",
        "  with open(output_path, \"w\") as outfile:\n",
        "\n",
        "    alchemy = 0\n",
        "    scene = 0\n",
        "    tangram = 0\n",
        "\n",
        "    for line in file:\n",
        "      ex = json.loads(line)\n",
        "      if 'ALCHEMY' in ex['id']:\n",
        "        alchemy += 1\n",
        "        if alchemy > samples:\n",
        "          continue\n",
        "      elif 'SCENE' in ex['id']:\n",
        "        scene += 1\n",
        "        if scene > samples:\n",
        "          continue\n",
        "      elif 'TANGRAM' in ex['id']:\n",
        "        tangram += 1\n",
        "        if tangram > samples:\n",
        "          continue\n",
        "        \n",
        "      json.dump(ex, outfile)\n",
        "      outfile.write('\\n')"
      ],
      "metadata": {
        "id": "BxcOjIf53f6C"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reduces scone datasets\n",
        "sconeCutter('/content/train_scone_interim.jsonl', 'train_scone.jsonl', 250)\n",
        "sconeCutter('/content/dev_scone_interim.jsonl', 'dev_scone.jsonl', 50)\n",
        "sconeCutter('/content/test_scone_interim.jsonl', 'test_scone.jsonl', 75)"
      ],
      "metadata": {
        "id": "XR41BpEo4tp6"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}